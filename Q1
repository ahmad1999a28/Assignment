1. Gradient Descent (GD): Computes and applies the gradient of the loss function using the entire training dataset (full batch) for one update.
2. Stochastic Gradient Descent (SGD): Updates model parameters after processing just one randomly chosen training example in each step.
3. Mini-Batch Gradient Descent: Updates model parameters using the average gradient computed over a small, randomized subset of the training data.
4. Momentum: Incorporates a fraction of the past update direction into the current update to accelerate convergence and dampen oscillations.
5. Nesterov Accelerated Gradient (NAG): Improves Momentum by calculating the gradient at a projected future position, leading to better trajectory correction.
6. Adagrad: Uses adaptive per-parameter learning rates, which are inversely proportional to the square root of the sum of their past squared gradients.
7. Adadelta: An adaptive learning rate method that restricts the window of accumulated past squared gradients to prevent the learning rate from diminishing too quickly.
8. RMSProp: Divides the learning rate by an exponentially decaying average of squared gradients to normalize the magnitude of the gradients.
9. Adam: An adaptive optimization algorithm that computes individual learning rates for different parameters, based on estimates of first and second moments of the gradients.
10. Adamax: A variation of Adam that utilizes a simpler update rule for the second moment, employing the infinity norm, which is robust for sparse gradients.
